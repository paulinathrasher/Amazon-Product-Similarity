{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l6IiwYDSkJu"
      },
      "source": [
        "From: Maya Johnson, Marketing Director\n",
        "Challenge: Our marketing team struggles to quickly understand sentiment across thousands of\n",
        "customer reviews. We need to identify products with sentiment that doesn’t match their star ratings and understand the language patterns that indicate customer satisfaction or dissatisfaction.\n",
        "Request: Build a model that can predict review ratings based on the review text. This would\n",
        "help us:\n",
        "• Identify products with mismatched ratings and sentiment\n",
        "• Extract key positive and negative product attributes\n",
        "• Develop more accurate messaging for different product types\n",
        "Success metrics: Rating prediction accuracy within 0.5 stars of the actual rating for at least\n",
        "75% of reviews, and identification of key phrases that predict high or low ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG2BvjXrYOfC",
        "outputId": "9757ac44-54ac-461f-fe4a-53469f6067b7"
      },
      "outputs": [],
      "source": [
        "!pip install textblob lightgbm --quiet\n",
        "import nltk\n",
        "nltk.download('punkt') #used by textblob to split sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9l8qbA7OAZL",
        "outputId": "e9f85c6c-71f8-4d92-c3c8-e1b5bb087d18"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive if needed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from textblob import TextBlob\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load preprocessing functions\n",
        "from de3_preprocessing import load_preprocessed, compute_tfidf, normalize_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "juscPnehOIkd",
        "outputId": "fc2fa98c-b555-4aca-8be0-c1ac70eafa7e"
      },
      "outputs": [],
      "source": [
        "# Load review text data and embeddings from preprocessed data\n",
        "df, embeddings = load_preprocessed(\"/content/drive/MyDrive/DEAssignment3/review\")\n",
        "\n",
        "# Quick check of the data\n",
        "print(df.columns)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "7m18DtXAgFPC",
        "outputId": "bf443a73-6235-4716-909d-08937d6fdaff"
      },
      "outputs": [],
      "source": [
        "# Plot rating distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x=\"rating\", data=df, palette=\"viridis\")\n",
        "plt.title(\"Distribution of Review Ratings\")\n",
        "plt.xlabel(\"Star Rating\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.xticks(ticks=[0, 1, 2, 3, 4], labels=[1, 2, 3, 4, 5])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print exact counts and proportions (optional for reference)\n",
        "rating_counts = df[\"rating\"].value_counts().sort_index()\n",
        "rating_proportions = df[\"rating\"].value_counts(normalize=True).sort_index()\n",
        "print(\"Rating counts:\\n\", rating_counts)\n",
        "print(\"\\nRating proportions:\\n\", rating_proportions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7T4CtiWlFuv"
      },
      "source": [
        "Sentiment Detection Model: cardiffnlp/twitter-roberta-base-sentiment from HuggingFace.\n",
        "\n",
        "This model is a version of RoBERTa that can classify text into positive, neutral, or negative sentiment classes. Each review is truncated to the first 500 characters because the model can only handle a limited sequence of characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gwA1q7yYa1a",
        "outputId": "b08d7d06-013d-4f6a-eb70-ecc0d064e3ee"
      },
      "outputs": [],
      "source": [
        "#takes a review and uses textblob to compute the polarity and subjectivity, outputs a pair of numbers\n",
        "\n",
        "from textblob import TextBlob\n",
        "!python -m textblob.download_corpora\n",
        "\n",
        "\n",
        "def compute_sentiment_smart(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentence_polarities = [sentence.sentiment.polarity for sentence in blob.sentences]\n",
        "\n",
        "    if not sentence_polarities:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    max_polarity = max(sentence_polarities)\n",
        "    min_polarity = min(sentence_polarities)\n",
        "    avg_polarity = np.mean(sentence_polarities)\n",
        "\n",
        "    final_polarity = (avg_polarity + (max_polarity if abs(max_polarity) > abs(min_polarity) else min_polarity)) / 2\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "    return final_polarity, subjectivity\n",
        "\n",
        "# Apply it\n",
        "df[\"polarity\"], df[\"subjectivity\"] = zip(*df[\"clean_text\"].apply(compute_sentiment_smart))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjN-f2UqT1g6"
      },
      "source": [
        "Similar to Challenge 1 - Numeric features review_length and helpfulness_score are added and normalized. MinMaxScaler normalizes values between 0 and 1. CSR_matrix used to store numeric features sparsely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCnGooK5OpeX"
      },
      "outputs": [],
      "source": [
        "# Compute TF-IDF features (based on cleaned text)\n",
        "tfidf = TfidfVectorizer(max_features=300) #converts text into a matrix of word importance\n",
        "tfidf_matrix = tfidf.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "# 2. Embeddings are already loaded (from earlier preprocessing)\n",
        "embedding_sparse = csr_matrix(embeddings)\n",
        "\n",
        "# 3. Numeric features: review length and helpfulness\n",
        "scaler = MinMaxScaler()\n",
        "numeric_features = scaler.fit_transform(df[[\"review_length\", \"helpful_vote\"]]) #adding numeric features review_length and helpfulness score\n",
        "numeric_sparse = csr_matrix(numeric_features)\n",
        "\n",
        "# Combine all features into one\n",
        "X_combined = hstack([tfidf_matrix, embedding_sparse, numeric_sparse])\n",
        "\n",
        "#embeddings are 384 dimensional vectors that capture context and meaning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvbNoaRCOthP"
      },
      "outputs": [],
      "source": [
        "# Target variable\n",
        "y = df[\"rating\"]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_combined, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dds5z_PZSoVR"
      },
      "source": [
        "Random forest was skipped to save time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvDSpZYhYx2l",
        "outputId": "3da1e31d-9755-4c3f-e5fc-53297dd731f4"
      },
      "outputs": [],
      "source": [
        "# Linear Regression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "y_pred_lin = lin_reg.predict(X_test)\n",
        "\n",
        "# # Random Forest Regressor\n",
        "# rf_reg = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "# rf_reg.fit(X_train, y_train)\n",
        "# y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "# LightGBM Regressor (new)\n",
        "lgb_reg = LGBMRegressor(n_estimators=100, random_state=42)\n",
        "lgb_reg.fit(X_train, y_train)\n",
        "y_pred_lgb = lgb_reg.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tghrmQU7Y1mI",
        "outputId": "219b0011-7549-4fd5-b5d3-f46da2a836f8"
      },
      "outputs": [],
      "source": [
        "def within_half_star(y_true, y_pred): #checks how often the predicted rating is with .5 stars of actual\n",
        "    return np.mean(np.abs(y_true - y_pred) <= 0.5) #returns percentage between 0-1 (the higher, the better)\n",
        "\n",
        "print(\"Linear Regression accuracy (within 0.5 stars):\", within_half_star(y_test, y_pred_lin))\n",
        "# print(\"Random Forest Regression accuracy (within 0.5 stars):\", within_half_star(y_test, y_pred_rf))\n",
        "print(\"LightGBM Regression accuracy (within 0.5 stars):\", within_half_star(y_test, y_pred_lgb))\n",
        "\n",
        "# Adding\n",
        "print(\"\\n=== Regression Performance Metrics ===\")\n",
        "print(\"Linear Regression MAE:\", mean_absolute_error(y_test, y_pred_lin)) #mean absolute error - on avg, how far off are predictions from true ratings\n",
        "print(\"Linear Regression R^2:\", r2_score(y_test, y_pred_lin)) #measures how well model can explain variance (1 = perfect, 0 = bad)\n",
        "\n",
        "print(\"LightGBM Regression MAE:\", mean_absolute_error(y_test, y_pred_lgb))\n",
        "print(\"LightGBM Regression R^2:\", r2_score(y_test, y_pred_lgb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrAQUFLtUmnY"
      },
      "source": [
        "Comparing Regression with Classifications - how would results differ if we treated this as a classification problem?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-Hap-U1S1rk",
        "outputId": "37c93fa4-207d-4d43-a914-3f37b7c6c0d5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nClassification report for LightGBM (rounded predictions):\")\n",
        "print(classification_report(y_test, np.round(y_pred_lgb)))\n",
        "\n",
        "print(\"\\nClassification report for LinReg(rounded predictions):\")\n",
        "print(classification_report(y_test, np.round(y_pred_lin)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZhTziTbTW5-"
      },
      "source": [
        "New Section: Classification modeling - predicting star ratings based on review features\n",
        "\n",
        "Models Used:\n",
        "\n",
        "\n",
        "*  Logistic Regression - trained using TF-IDF and other engineered features\n",
        "*   SVC - trained to find optimal hyper plane to seperate different rating classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC2ZBud8Gyj4",
        "outputId": "c59f759f-d30a-4d6f-c3dc-2e728fc62e3c"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# 1. Create classification labels\n",
        "#reusing rating column as labels\n",
        "\n",
        "# Training a logistic regression model\n",
        "log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_log = log_clf.predict(X_test) #logreg's predictions\n",
        "\n",
        "print(\"\\n=== Logistic Regression Classifier Results ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log))\n",
        "print(classification_report(y_test, y_pred_log))\n",
        "\n",
        "# training a SVC - finds best decision boundary to separate classes\n",
        "  #predicts review rating based on review features\n",
        "svc_clf = SVC(kernel=\"linear\", random_state=42) #using linear kernel, assuming data can be separated w straight line\n",
        "svc_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svc = svc_clf.predict(X_test)\n",
        "\n",
        "print(\"\\n=== Support Vector Classifier Results ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svc))\n",
        "print(classification_report(y_test, y_pred_svc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IDlhQxJUBXZ"
      },
      "source": [
        "The next section is used to identify the most influential words that drive a classification as positive or negative\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I0noSHaKN65i",
        "outputId": "653fa608-4688-4a19-d226-0ac470379916"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# New Lexicon Extraction using Logistic Regression coefficients\n",
        "\n",
        "#creating a binary label for sentiment: 1 = positive (4-5 stars), 0 = negative (1-2 stars)\n",
        "df[\"positive_sentiment\"] = df[\"rating\"].apply(lambda x: 1 if x >= 4 else 0)\n",
        "\n",
        "mask = df[\"rating\"] != 3\n",
        "X_text = tfidf_matrix[mask]\n",
        "y_sentiment = df.loc[mask, \"positive_sentiment\"]\n",
        "\n",
        "# 2. Train logistic regression\n",
        "lexicon_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lexicon_clf.fit(X_text, y_sentiment)\n",
        "\n",
        "# 3. Extract top positive and negative words based on coefficients\n",
        "feature_names = np.array(tfidf.get_feature_names_out())\n",
        "coefs = lexicon_clf.coef_[0]\n",
        "\n",
        "# Sort words\n",
        "top_positive_indices = np.argsort(coefs)[-15:]  # Top 15 positive\n",
        "top_negative_indices = np.argsort(coefs)[:15]   # Top 15 negative\n",
        "\n",
        "positive_terms = pd.DataFrame({\n",
        "    \"term\": feature_names[top_positive_indices],\n",
        "    \"coef\": coefs[top_positive_indices]\n",
        "}).sort_values(by=\"coef\", ascending=False)\n",
        "\n",
        "negative_terms = pd.DataFrame({\n",
        "    \"term\": feature_names[top_negative_indices],\n",
        "    \"coef\": coefs[top_negative_indices]\n",
        "}).sort_values(by=\"coef\")\n",
        "\n",
        "# Print\n",
        "print(\"\\n=== Top Positive Sentiment Terms ===\")\n",
        "print(positive_terms)\n",
        "\n",
        "print(\"\\n=== Top Negative Sentiment Terms ===\")\n",
        "print(negative_terms)\n",
        "\n",
        "# 4. Plot together\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_terms = pd.concat([positive_terms, negative_terms])\n",
        "sns.barplot(data=top_terms, x=\"coef\", y=\"term\", palette=\"coolwarm\")\n",
        "plt.title(\"Top Words Predicting Positive vs Negative Sentiment\")\n",
        "plt.xlabel(\"Logistic Regression Coefficient\")\n",
        "plt.ylabel(\"Term\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjC1X5c3WHCE"
      },
      "source": [
        "Sentiment vs Rating Mismatch Detection- This code checks for mismatches between a review's text sentiment and its star rating.\n",
        "Specifically, it flags reviews that:\n",
        "\n",
        "Sound positive (high polarity) but have a low star rating (1–2 stars)\n",
        "\n",
        "Sound negative (low polarity) but have a high star rating (4–5 stars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "36J7D9mCWFVj",
        "outputId": "2f960420-6755-4e04-f7de-fd4f8a948e4a"
      },
      "outputs": [],
      "source": [
        "# Define mismatch based on TextBlob polarity\n",
        "# - Positive polarity (> 0.4) but low rating (1 or 2)\n",
        "# - Negative polarity (< -0.4) but high rating (4 or 5)\n",
        "df[\"mismatch\"] = ((df[\"polarity\"] > 0.4) & (df[\"rating\"] <= 2)) | \\\n",
        "                 ((df[\"polarity\"] < -0.4) & (df[\"rating\"] >= 4))\n",
        "\n",
        "# Show how many mismatches\n",
        "print(f\"Total mismatches found: {df['mismatch'].sum()} / {len(df)} reviews\")\n",
        "\n",
        "# Display more mismatched examples with longer text shown\n",
        "print(\"\\nExamples of mismatched reviews:\")\n",
        "mismatched_reviews = df[df[\"mismatch\"]][[\"text\", \"rating\", \"polarity\"]]\n",
        "\n",
        "# Optional: display longer text\n",
        "pd.set_option(\"display.max_colwidth\", None)  # disables truncation\n",
        "\n",
        "# Sample and show more mismatches\n",
        "display(mismatched_reviews.sample(10, random_state=42))  # adjust 10 if you want\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCMg8JDYZPPW",
        "outputId": "02820e8f-ab4e-4907-9923-542d070d57bf"
      },
      "outputs": [],
      "source": [
        "# Group mismatches by product (parent_asin)\n",
        "product_mismatch_counts = df[df[\"mismatch\"]].groupby(\"parent_asin\").size().sort_values(ascending=False)\n",
        "\n",
        "# Create a DataFrame\n",
        "product_mismatch_df = product_mismatch_counts.reset_index()\n",
        "product_mismatch_df.columns = [\"parent_asin\", \"mismatch_count\"]\n",
        "\n",
        "# Bring in just category (drop title)\n",
        "product_info = df[[\"parent_asin\", \"category\"]].drop_duplicates(subset=\"parent_asin\")\n",
        "\n",
        "# Merge counts with category info\n",
        "product_mismatch_df = product_mismatch_df.merge(product_info, on=\"parent_asin\", how=\"left\")\n",
        "\n",
        "# Top 20 products with the most mismatches\n",
        "print(\"\\n=== Top 20 Products with Most Sentiment/Rating Mismatches (with category) ===\")\n",
        "print(product_mismatch_df.head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLBpUgGWppr"
      },
      "source": [
        "Category-Level Predictability - for each category, how does ratiing predictability change?\n",
        "\n",
        "Similar analysis to above, just breaking it out by category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ECBaQ-gcVCu4",
        "outputId": "66332ff3-25ac-4bb1-93f2-c20ba1807ba1"
      },
      "outputs": [],
      "source": [
        "# initializing empty dataframe\n",
        "category_results = []\n",
        "\n",
        "# Looping through each category separately\n",
        "for category in df[\"category\"].unique():\n",
        "    # Select reviews from this category\n",
        "    cat_mask = df[\"category\"] == category\n",
        "    X_cat = X_combined[cat_mask]\n",
        "    y_cat = df.loc[cat_mask, \"rating\"]\n",
        "\n",
        "    # Split into train/test for this category\n",
        "    X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(\n",
        "        X_cat, y_cat, test_size=0.2, random_state=42, stratify=y_cat\n",
        "    )\n",
        "\n",
        "    # Train a simple model (LightGBM, like before\n",
        "    model_cat = LGBMRegressor(n_estimators=50, random_state=42)\n",
        "    model_cat.fit(X_train_cat, y_train_cat)\n",
        "    y_pred_cat = model_cat.predict(X_test_cat)\n",
        "\n",
        "    # Evaluate accuracy within 0.5 stars\n",
        "    acc = within_half_star(y_test_cat, y_pred_cat)\n",
        "\n",
        "    # Save results\n",
        "    category_results.append({\n",
        "        \"category\": category,\n",
        "        \"accuracy_within_0.5_star\": acc\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame and show\n",
        "category_results_df = pd.DataFrame(category_results).sort_values(by=\"accuracy_within_0.5_star\", ascending=False)\n",
        "print(\"\\n=== Predictability by Product Category ===\")\n",
        "print(category_results_df)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=category_results_df, x=\"accuracy_within_0.5_star\", y=\"category\")\n",
        "plt.title(\"Rating Predictability by Product Category\")\n",
        "plt.xlabel(\"Accuracy within 0.5 stars\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qteyR9WHXEXQ"
      },
      "source": [
        "Review length and complexity analysis\n",
        "\n",
        "Model Used\n",
        "\n",
        "\n",
        "*   Light GBM Regressor - boosted decision tree model. Builds a series of decision trees, where each new tree tries to correct mistakes from previous\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c7eNGV0NXGDl",
        "outputId": "cd795a2e-07d6-4363-8626-0eeb71fc67fc"
      },
      "outputs": [],
      "source": [
        "#making bins for review length\n",
        "df[\"length_bin\"] = pd.cut(df[\"review_length\"], bins=[0, 25, 50, 100, 200, np.inf],\n",
        "                          labels=[\"0-25 words\", \"26-50 words\", \"51-100 words\", \"101-200 words\", \"200+ words\"])\n",
        "\n",
        "# for each bin, analyze a subset of the reviews.\n",
        "length_results = []\n",
        "\n",
        "for bin_label in df[\"length_bin\"].unique():\n",
        "    bin_mask = df[\"length_bin\"] == bin_label\n",
        "    X_bin = X_combined[bin_mask]\n",
        "    y_bin = df.loc[bin_mask, \"rating\"]\n",
        "\n",
        "    if len(y_bin) < 100:  # Skip the smallest groups\n",
        "        continue\n",
        "\n",
        "    X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
        "        X_bin, y_bin, test_size=0.2, random_state=42, stratify=y_bin\n",
        "    )\n",
        "\n",
        "    model_bin = LGBMRegressor(n_estimators=50, random_state=42) #LightGBM regressor model used\n",
        "    model_bin.fit(X_train_bin, y_train_bin)\n",
        "    y_pred_bin = model_bin.predict(X_test_bin)\n",
        "\n",
        "    acc = within_half_star(y_test_bin, y_pred_bin)\n",
        "\n",
        "    length_results.append({\n",
        "        \"length_bin\": bin_label,\n",
        "        \"accuracy_within_0.5_star\": acc\n",
        "    })\n",
        "\n",
        "length_results_df = pd.DataFrame(length_results).sort_values(by=\"accuracy_within_0.5_star\", ascending=False)\n",
        "print(\"\\n=== Predictability by Review Length ===\")\n",
        "print(length_results_df)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=length_results_df, x=\"accuracy_within_0.5_star\", y=\"length_bin\")\n",
        "plt.title(\"Rating Predictability by Review Length\")\n",
        "plt.xlabel(\"Accuracy within 0.5 stars\")\n",
        "plt.ylabel(\"Review Length Bin\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
