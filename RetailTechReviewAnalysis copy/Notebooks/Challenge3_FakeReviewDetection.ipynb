{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-Oca99Qquar"
      },
      "source": [
        "Challenge: We’ve observed an increase in suspected fake reviews on our platform. These reviews\n",
        "mislead consumers and damage our brand reputation. We need a systematic way to identify\n",
        "potentially fake reviews for further investigation.\n",
        "Request: Develop a classification system that can flag potentially artificial or inauthentic reviews. The system should:\n",
        "• Identify patterns characteristic of fake reviews\n",
        "• Have a low false positive rate to avoid flagging legitimate reviews\n",
        "• Provide explainable results that our team can use for investigations\n",
        "Success metrics: High precision in identifying suspicious reviews and clear explanation of the\n",
        "patterns that trigger flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xabs_UxYsTl8",
        "outputId": "70d2f834-d48b-463f-8695-04ea1e180b18"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "yPkDgVZZprLL",
        "outputId": "f98d89b5-0d12-4eab-d42f-a547bc424752"
      },
      "outputs": [],
      "source": [
        "# Load real cleaned reviews\n",
        "\n",
        "real_reviews = pd.read_pickle(\"/content/drive/MyDrive/DEAssignment3/review_review_df_with_clean_text.pkl\")\n",
        "\n",
        "# Checking first few rows\n",
        "print(f\"Loaded {len(real_reviews)} real reviews.\")\n",
        "real_reviews.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy1FnXpMsW-F"
      },
      "source": [
        "Creating fake reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1wP8EFEsYun"
      },
      "outputs": [],
      "source": [
        "# Components to build fake reviews\n",
        "positive_adjectives = [\n",
        "    \"amazing\", \"fantastic\", \"incredible\", \"perfect\", \"outstanding\", \"good\", \"great\", \"fabulous\",\n",
        "    \"heartwarming\", \"brilliant\", \"exceptional\", \"lovely\", \"cute\", \"adorable\", \"touching\", \"funny\",\n",
        "    \"gorgeous\", \"spectacular\", \"breathtaking\", \"remarkable\", \"impressive\", \"charming\", \"delightful\",\n",
        "    \"memorable\", \"beautiful\", \"stellar\",\"weird\",\"okay\"\n",
        "]\n",
        "\n",
        "negative_adjectives = [\n",
        "    \"terrible\", \"horrible\", \"disappointing\", \"poor\", \"awful\", \"bad\", \"gross\", \"cheap\", \"flimsy\",\n",
        "    \"brutal\", \"unreliable\", \"uncomfortable\", \"mediocre\", \"careless\", \"ignorant\", \"nasty\", \"finicky\",\n",
        "    \"underwhelming\", \"shoddy\", \"incomplete\", \"faulty\", \"dull\", \"boring\", \"broken\", \"dirty\",\"weird\",\"okay\"\n",
        "]\n",
        "\n",
        "nouns = [\n",
        "    \"product\", \"item\", \"purchase\", \"experience\", \"quality\", \"gift\", \"thing\", \"present\", \"tool\",\n",
        "    \"pair\", \"set\", \"device\", \"investment\", \"choice\", \"addition\", \"purchase decision\", \"gadget\",\n",
        "    \"essential\", \"accessory\"\n",
        "]\n",
        "\n",
        "positive_verbs = [\n",
        "    \"loved\", \"enjoyed\", \"recommended\", \"appreciated\", \"valued\", \"liked\", \"gifted\", \"bought\",\n",
        "    \"admired\", \"smiled\", \"treasured\", \"swooned\", \"laughed\", \"celebrated\", \"endorsed\", \"relished\",\n",
        "    \"embraced\", \"wowed by\", \"was thrilled with\",\"satisfied\"\n",
        "]\n",
        "\n",
        "negative_verbs = [\n",
        "    \"hated\", \"disliked\", \"regretted\", \"returned\", \"complained about\", \"waited\", \"struggled with\",\n",
        "    \"abandoned\", \"wished I hadn't bought\", \"misled\", \"detested\", \"suffered through\", \"was disappointed by\",\n",
        "    \"fought with\", \"fumbled through\", \"was annoyed by\", \"questioned buying\",\"satisfied\"\n",
        "]\n",
        "\n",
        "extra_positive_sentences = [\n",
        "    \"Shipping was quick.\",\n",
        "    \"Would definitely buy again.\",\n",
        "    \"Exceeded my expectations.\",\n",
        "    \"This made the perfect gift.\",\n",
        "    \"Absolutely worth every penny.\",\n",
        "    \"Five stars without hesitation.\",\n",
        "    \"Made my day better!\",\n",
        "    \"Loved it so much!\",\n",
        "    \"Thoroughly enjoyed this a lot.\",\n",
        "    \"High quality through and through.\",\n",
        "    \"It brought a smile to my face.\",\n",
        "    \"Can't stop recommending it!\",\n",
        "    \"Feels like a luxury purchase.\",\n",
        "    \"Brightened up my whole week.\",\n",
        "    \"Everything about it was perfect.\"\n",
        "]\n",
        "\n",
        "extra_negative_sentences = [\n",
        "    \"Not worth the money.\",\n",
        "    \"Fell apart after one use.\",\n",
        "    \"Extremely disappointed.\",\n",
        "    \"Would not recommend to anyone.\",\n",
        "    \"Regret buying this.\",\n",
        "    \"Waste of time and money.\",\n",
        "    \"Very cheaply made.\",\n",
        "    \"It was just okay.\",\n",
        "    \"Will not be re-purchasing.\",\n",
        "    \"Disappointed in this one.\",\n",
        "    \"Broke way too easily.\",\n",
        "    \"Customer service was awful.\",\n",
        "    \"Expected much better for the price.\",\n",
        "    \"Would give zero stars if I could.\",\n",
        "    \"Never buying from here again.\"\n",
        "]\n",
        "\n",
        "# Function to generate one fake review\n",
        "def generate_fake_review(sentiment=\"positive\"):\n",
        "    adj = random.choice(positive_adjectives if sentiment == \"positive\" else negative_adjectives)\n",
        "    noun = random.choice(nouns)\n",
        "    verb = random.choice(positive_verbs if sentiment == \"positive\" else negative_verbs)\n",
        "\n",
        "    # 1. Create first sentence - now with more variety\n",
        "    templates = [\n",
        "        f\"I {verb} this {noun}. It is absolutely {adj}.\",\n",
        "        f\"This {noun} was {adj}! I {verb} it.\",\n",
        "        f\"Such a {adj} {noun}! I {verb} it.\",\n",
        "        f\"I {verb} the {noun}. Truly {adj}.\",\n",
        "        f\"My {noun} experience was {adj}.\",\n",
        "        f\"I can't believe how {adj} this {noun} was!\",\n",
        "        f\"I was really {adj} with this {noun}.\",\n",
        "        f\"This {noun} completely {verb} me!\",\n",
        "        f\"My experience with this {noun} was simply {adj}.\",\n",
        "        f\"I never expected a {noun} to be so {adj}!\",\n",
        "        f\"This {noun} exceeded all my expectations. Truly {adj}.\",\n",
        "        f\"I {verb} my {noun}, it was {adj}!\",\n",
        "        f\"Honestly, this {noun} was just {adj}.\",\n",
        "        f\"I found the {noun} to be {adj} overall.\",\n",
        "        f\"It's a {adj} {noun}, that's for sure.\",\n",
        "    ]\n",
        "\n",
        "    # Occasionally (10% chance) drop the adjective for realism\n",
        "    if random.random() < 0.1:\n",
        "        templates.append(f\"I {verb} this {noun}.\")  # Basic simple sentence\n",
        "\n",
        "    review = random.choice(templates)\n",
        "\n",
        "    # 2. Optional \"but\" clause for realism (small chance, ~15%)\n",
        "    if random.random() < 0.15:\n",
        "        but_phrases = [\n",
        "            \"but shipping took a while.\",\n",
        "            \"but packaging could be better.\",\n",
        "            \"but it was smaller than expected.\",\n",
        "            \"but setup was a bit tricky.\",\n",
        "            \"but color was slightly off.\",\n",
        "            \"but it didn't last as long as I hoped.\",\n",
        "            \"but it wasn't exactly what I expected.\",\n",
        "            \"but it felt a little cheap.\",\n",
        "            \"but the instructions were confusing.\",\n",
        "            \"but customer service could have been better.\",\n",
        "            \"but the price was a little high.\",\n",
        "        ]\n",
        "        review += \" \" + random.choice(but_phrases)\n",
        "\n",
        "    # 3. Add 1–3 extra positive or negative sentences\n",
        "    if sentiment == \"positive\":\n",
        "        extras = random.sample(extra_positive_sentences, k=random.choice([1, 2, 3]))\n",
        "    else:\n",
        "        extras = random.sample(extra_negative_sentences, k=random.choice([1, 2, 3]))\n",
        "\n",
        "    review += \" \" + \" \".join(extras)\n",
        "\n",
        "    return review\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I2X9BDW7ZCB",
        "outputId": "4013e326-23b7-49ef-dcb0-3c5960d6dbe0"
      },
      "outputs": [],
      "source": [
        "# Preview 10 random fake reviews\n",
        "print(\"\\n=== Preview of 10 Generated Fake Reviews ===\")\n",
        "for _ in range(10):\n",
        "    sentiment = random.choice([\"positive\", \"negative\"])\n",
        "    print(f\"({sentiment.title()}) {generate_fake_review(sentiment)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "B2oGaUmxsgZf",
        "outputId": "0e7ae2dc-3a76-4485-c7fe-bbb7481180c4"
      },
      "outputs": [],
      "source": [
        "#Labeling real reviews and combining\n",
        "\n",
        "# adding \"is_fake\" column to real reviews (real = 0)\n",
        "real_reviews[\"is_fake\"] = 0\n",
        "\n",
        "# Select only necessary columns (matching structure)\n",
        "real_reviews_subset = real_reviews[[\"clean_text\", \"rating\", \"parent_asin\", \"category\", \"is_fake\"]]\n",
        "\n",
        "# Generate fake reviews\n",
        "fake_reviews = []\n",
        "\n",
        "categories = real_reviews[\"category\"].unique()\n",
        "for category in categories:\n",
        "    for _ in range(100):  # 100 fake reviews per category\n",
        "        sentiment = random.choice([\"positive\", \"negative\"])\n",
        "        review_text = generate_fake_review(sentiment=sentiment)\n",
        "        rating = random.choice([1, 2, 3, 4, 5])\n",
        "        parent_asin = f\"FAKE-{random.randint(10000,99999)}\"  # made-up ID\n",
        "\n",
        "        fake_reviews.append({\n",
        "            \"clean_text\": review_text,\n",
        "            \"rating\": rating,\n",
        "            \"parent_asin\": parent_asin,\n",
        "            \"category\": category,\n",
        "            \"is_fake\": 1\n",
        "        })\n",
        "\n",
        "# Turn into DataFrame\n",
        "fake_reviews_df = pd.DataFrame(fake_reviews)\n",
        "\n",
        "# Combine real + fake\n",
        "combined_reviews = pd.concat([real_reviews_subset, fake_reviews_df], ignore_index=True)\n",
        "\n",
        "print(f\"Combined dataset now has {len(combined_reviews)} reviews.\")\n",
        "combined_reviews.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzaULNHEIaUp",
        "outputId": "bfe4bfd4-4d72-49f2-edd0-46c5c60be65c"
      },
      "outputs": [],
      "source": [
        "#Saving fake reviews to a csv:\n",
        "\n",
        "fake_reviews_df.to_csv(\"/content/fake_reviews.csv\", index=False)\n",
        "\n",
        "print(\"✅ Fake reviews saved to fake_reviews.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsDe_OWlvN4O"
      },
      "outputs": [],
      "source": [
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=300)\n",
        "\n",
        "# Fit and transform the review text\n",
        "X_text = tfidf.fit_transform(combined_reviews[\"clean_text\"])\n",
        "\n",
        "# Target variable\n",
        "y = combined_reviews[\"is_fake\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybqL3vDwvPJO",
        "outputId": "743b9460-293e-402d-b063-114b5be50447"
      },
      "outputs": [],
      "source": [
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Training samples:\", X_train.shape[0])\n",
        "print(\"Testing samples:\", X_test.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a-WDZrbvpn_",
        "outputId": "6156239b-a81f-4d40-b7e4-aaabcc3b14f3"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42, class_weight = \"balanced\")\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_log = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\n=== Logistic Regression Results ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_log))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_log))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log))\n",
        "print(classification_report(y_test, y_pred_log))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFRTL0AHvrJe",
        "outputId": "63ca4978-5803-46f8-d794-69ebd85d269a"
      },
      "outputs": [],
      "source": [
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42,class_weight = \"balanced\")\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\n=== Random Forest Classifier Results ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_rf))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmtK20ppWXut"
      },
      "source": [
        "Performance split out by category\n",
        "- Predicted probabilities of being fake and hard predictions are made using 0.5 threshold\n",
        "- Accuracy, precision and recall are calculated for each category and results are compiled in a single df\n",
        "-Results are visualized in bar plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ehZEUWiP4EPw",
        "outputId": "e8726108-4e1c-4cc4-bdd8-23e3362277c0"
      },
      "outputs": [],
      "source": [
        "# Add category info back to test set\n",
        "X_test_indices = X_test.nonzero()[0]\n",
        "test_categories = combined_reviews.iloc[y_test.index][\"category\"].values\n",
        "\n",
        "# Predict probabilities and predictions\n",
        "y_proba_log = log_reg.predict_proba(X_test)[:, 1]  # probability of being fake\n",
        "y_pred_log = (y_proba_log > 0.5).astype(int)        # threshold at 0.5\n",
        "\n",
        "# Analyze performance per category - for each category, assess accuracy, precision and recall of predictions\n",
        "category_metrics = []\n",
        "\n",
        "for category in np.unique(test_categories):\n",
        "    mask = test_categories == category\n",
        "    y_true_cat = y_test[mask]\n",
        "    y_pred_cat = y_pred_log[mask]\n",
        "\n",
        "    accuracy = accuracy_score(y_true_cat, y_pred_cat)\n",
        "    precision = precision_score(y_true_cat, y_pred_cat, zero_division=0)\n",
        "    recall = recall_score(y_true_cat, y_pred_cat, zero_division=0)\n",
        "\n",
        "    category_metrics.append({\n",
        "        \"category\": category,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "category_results_df = pd.DataFrame(category_metrics).sort_values(by=\"accuracy\", ascending=False)\n",
        "\n",
        "# Results Display\n",
        "print(\"\\n=== Performance by Product Category ===\")\n",
        "print(category_results_df)\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=category_results_df, x=\"accuracy\", y=\"category\")\n",
        "plt.title(\"Fake Review Detection Accuracy by Category\")\n",
        "plt.xlabel(\"Accuracy\")\n",
        "plt.ylabel(\"Product Category\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=category_results_df, x=\"precision\", y=\"category\")\n",
        "plt.title(\"Fake Review Detection Precision by Category\")\n",
        "plt.xlabel(\"Precision\")\n",
        "plt.ylabel(\"Product Category\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U0g2X-rh4r5A",
        "outputId": "d1fc82a9-7db5-4350-b33d-7fe5af810db2"
      },
      "outputs": [],
      "source": [
        "#Feature importance for Logistic Regression\n",
        "#Get feature names from TF-IDF\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "# Get coefficients - this tells us how influential each feature is\n",
        "coefs = log_reg.coef_[0]\n",
        "\n",
        "# Create a DataFrame\n",
        "coef_df = pd.DataFrame({\n",
        "    \"term\": feature_names,\n",
        "    \"coefficient\": coefs\n",
        "})\n",
        "\n",
        "# Sort by impact greatest to lowest\n",
        "top_positive = coef_df.sort_values(by=\"coefficient\", ascending=False).head(15)  # words that strongly predict fake\n",
        "top_negative = coef_df.sort_values(by=\"coefficient\", ascending=True).head(15)   # words that strongly predict real\n",
        "\n",
        "# 5. Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=pd.concat([top_positive, top_negative]), x=\"coefficient\", y=\"term\", palette=\"coolwarm\")\n",
        "plt.title(\"Top Words Predicting Fake vs Real Reviews\")\n",
        "plt.xlabel(\"Coefficient (positive = more fake, negative = more real)\")\n",
        "plt.ylabel(\"Term\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#Printing top ten terms\n",
        "print(\"\\n=== Top Positive (Fake) Indicators ===\")\n",
        "print(top_positive)\n",
        "\n",
        "print(\"\\n=== Top Negative (Real) Indicators ===\")\n",
        "print(top_negative)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH5gUtFWVzwn"
      },
      "source": [
        "Threshold Analysis - generate the predicted probability that a review is fake using different thresholds to adjust the sensitivity of the fake review detection system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0LNONM3WIHLH",
        "outputId": "95889475-2315-4c63-8d72-85991d37984d"
      },
      "outputs": [],
      "source": [
        "# Get predicted probabilities\n",
        "y_probs = log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Define thresholds\n",
        "thresholds = [0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "# Lists to store metrics\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "\n",
        "# Loop through thresholds and store metrics\n",
        "for thresh in thresholds:\n",
        "    y_pred_thresh = (y_probs >= thresh).astype(int)\n",
        "\n",
        "    precision = precision_score(y_test, y_pred_thresh) #correctness of flagged fakes\n",
        "    recall = recall_score(y_test, y_pred_thresh) #how many true fakes are caught\n",
        "\n",
        "    precision_scores.append(precision)\n",
        "    recall_scores.append(recall)\n",
        "\n",
        "    print(f\"\\n=== Threshold: {thresh} ===\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred_thresh))\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_thresh))\n",
        "\n",
        "# 5. Plot precision and recall\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(thresholds, precision_scores, marker='o', label='Precision')\n",
        "plt.plot(thresholds, recall_scores, marker='s', label='Recall')\n",
        "plt.title('Precision and Recall vs Threshold')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(thresholds)\n",
        "plt.ylim(0, 1.05)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
